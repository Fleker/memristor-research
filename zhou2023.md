# [Bring memristive in-memory computing into general-purpose machine learning: A perspective](https://pubs.aip.org/aip/aml/article/1/4/040901/2916099/Bring-memristive-in-memory-computing-into-general)

## Notes
- Advocates for in-memory computing
- "as resistive random-access memory (RRAM),8 phase change mem-
ory (PCM),9 ferroelectric RAM (FeRAM),11 and magnetic RAM (MRAM),12"
- "In 2020, a fully hardware-implemented CNN18 was demonstrated for the first time using memristors. It achieved software-comparable classification accuracy and nearly a 100-fold improvement in energy efficiency compared with the Nvidia V100 platform."
- Most of the citation numbers in the paper seem to be one number greater based on contexts

> Significant issues for memristive IMC-based GPML involve the
> solutions of two inherent conflicts: algorithmic completeness and
> precision reconfigurability. Although memristive memories have
> proven their potential for logical operations, an all-in-one logical
> machine for storage and computing is still in the proof-of-concept
> stage.66 The main shortcomings are processing speed,67,68 limited
> endurance of the device,67 and logic cascading.69

- "Meanwhile, more attention has been focused on improving the pre-
cision of storage in arrays. One important solution is to increase the intermediate conductance levels of the memristive device. In fact, one such device has been reported to achieve 11-bit middle states.108"
    - 11-bit => 2048 distinct values
- "Many CPU- and GPU-based ML applications have been created using mature ML frameworks, such as PyTorch and TensorFlow, making application development easier. However, there are still no public or commercial libraries that support the development of top-level IMC applications, for example, IMC-supported PyTorch."
- "Third, it is the need to design the instruction set architecture (ISA), which abstracts the hardware, to construct the basis for running the high-level codes on the IMC systems."
- "work was also supported by the Hubei Engineering Research Center on Microelectronics and **Chua Memristor Institute**."

## Thesis Notes
- Include some of the hardware-related history
- Point out endurance and stress the need for speed

## References

```
@article{10.1063/5.0167743,
    author = {Zhou, Houji and Chen, Jia and Li, Jiancong and Yang, Ling and Li, Yi and Miao, Xiangshui},
    title = "{Bring memristive in-memory computing into general-purpose machine learning: A perspective}",
    journal = {APL Machine Learning},
    volume = {1},
    number = {4},
    pages = {040901},
    year = {2023},
    month = {10},
    abstract = "{In-memory computing (IMC) using emerging nonvolatile devices has received considerable attention due to its great potential for accelerating artificial neural networks and machine learning tasks. As the basic concept and operation modes of IMC are now well established, there is growing interest in employing its wide and general application. In this perspective, the path that leads memristive IMC to general-purpose machine learning is discussed in detail. First, we reviewed the development timeline of machine learning algorithms that employ memristive devices, such as resistive random-access memory and phase-change memory. Then we summarized two typical aspects of realizing IMC-based general-purpose machine learning. One involves a heterogeneous computing system for algorithmic completeness. The other is to obtain the configurable precision techniques for the compromise of the precision-efficiency dilemma. Finally, the major directions and challenges of memristive IMC-based general-purpose machine learning are proposed from a cross-level design perspective.}",
    issn = {2770-9019},
    doi = {10.1063/5.0167743},
    url = {https://doi.org/10.1063/5.0167743},
    eprint = {https://pubs.aip.org/aip/aml/article-pdf/doi/10.1063/5.0167743/18198793/040901\_1\_5.0167743.pdf},
}
```

Number 17 Impl paper. (The paper has the wrong citation)
https://www.nature.com/articles/s41586-020-1942-4

```
@Article{Yao2020,
author={Yao, Peng
and Wu, Huaqiang
and Gao, Bin
and Tang, Jianshi
and Zhang, Qingtian
and Zhang, Wenqiang
and Yang, J. Joshua
and Qian, He},
title={Fully hardware-implemented memristor convolutional neural network},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7792},
pages={641-646},
abstract={Memristor-enabled neuromorphic computing systems provide a fast and energy-efficient approach to training neural networks1--4. However, convolutional neural networks (CNNs)---one of the most important models for image recognition5---have not yet been fully hardware-implemented using memristor crossbars, which are cross-point arrays with a memristor device at each intersection. Moreover, achieving software-comparable results is highly challenging owing to the poor yield, large variation and other non-ideal characteristics of devices6--9. Here we report the fabrication of high-yield, high-performance and uniform memristor crossbar arrays for the implementation of CNNs, which integrate eight 2,048-cell memristor arrays to improve parallel-computing efficiency. In addition, we propose an effective hybrid-training method to adapt to device imperfections and improve the overall system performance. We built a five-layer memristor-based CNN to perform MNIST10 image recognition, and achieved a high accuracy of more than 96 per cent. In addition to parallel convolutions using different kernels with shared inputs, replication of multiple identical kernels in memristor arrays was demonstrated for processing different inputs in parallel. The memristor-based CNN neuromorphic system has an energy efficiency more than two orders of magnitude greater than that of state-of-the-art graphics-processing units, and is shown to be scalable to larger networks, such as residual neural networks. Our results are expected to enable a viable memristor-based non-von Neumann hardware solution for deep neural networks and edge computing.},
issn={1476-4687},
doi={10.1038/s41586-020-1942-4},
url={https://doi.org/10.1038/s41586-020-1942-4}
}
```

Here is an integrated chip for RRAM operations: https://www.nature.com/articles/s41586-022-04992-8
```
@Article{Wan2022,
author={Wan, Weier
and Kubendran, Rajkumar
and Schaefer, Clemens
and Eryilmaz, Sukru Burc
and Zhang, Wenqiang
and Wu, Dabin
and Deiss, Stephen
and Raina, Priyanka
and Qian, He
and Gao, Bin
and Joshi, Siddharth
and Wu, Huaqiang
and Wong, H.-S. Philip
and Cauwenberghs, Gert},
title={A compute-in-memory chip based on resistive random-access memory},
journal={Nature},
year={2022},
month={Aug},
day={01},
volume={608},
number={7923},
pages={504-512},
abstract={Realizing increasingly complex artificial intelligence (AI) functionalities directly on edge devices calls for unprecedented energy efficiency of edge hardware. Compute-in-memory (CIM) based on resistive random-access memory (RRAM)1 promises to meet such demand by storing AI model weights in dense, analogue and non-volatile RRAM devices, and by performing AI computation directly within RRAM, thus eliminating power-hungry data movement between separate compute and memory2--5. Although recent studies have demonstrated in-memory matrix-vector multiplication on fully integrated RRAM-CIM hardware6--17, it remains a goal for a RRAM-CIM chip to simultaneously deliver high energy efficiency, versatility to support diverse models and software-comparable accuracy. Although efficiency, versatility and accuracy are all indispensable for broad adoption of the technology, the inter-related trade-offs among them cannot be addressed by isolated improvements on any single abstraction level of the design. Here, by co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM---a RRAM-based CIM chip that simultaneously delivers versatility in reconfiguring CIM cores for diverse model architectures, energy efficiency that is two-times better than previous state-of-the-art RRAM-CIM chips across various computational bit-precisions, and inference accuracy comparable to software models quantized to four-bit weights across various AI tasks, including accuracy of 99.0{\thinspace}percent on MNIST18 and 85.7{\thinspace}percent on CIFAR-1019 image classification, 84.7-percent accuracy on Google speech command recognition20, and a 70-percent reduction in image-reconstruction error on a Bayesian image-recovery task.},
issn={1476-4687},
doi={10.1038/s41586-022-04992-8},
url={https://doi.org/10.1038/s41586-022-04992-8}
}
```

11-bit Conductance Levels: https://www.nature.com/articles/s41586-023-05759-5

```
ï»¿@Article{Rao2023,
author={Rao, Mingyi
and Tang, Hao
and Wu, Jiangbin
and Song, Wenhao
and Zhang, Max
and Yin, Wenbo
and Zhuo, Ye
and Kiani, Fatemeh
and Chen, Benjamin
and Jiang, Xiangqi
and Liu, Hefei
and Chen, Hung-Yu
and Midya, Rivu
and Ye, Fan
and Jiang, Hao
and Wang, Zhongrui
and Wu, Mingche
and Hu, Miao
and Wang, Han
and Xia, Qiangfei
and Ge, Ning
and Li, Ju
and Yang, J. Joshua},
title={Thousands of conductance levels in memristors integrated on CMOS},
journal={Nature},
year={2023},
month={Mar},
day={01},
volume={615},
number={7954},
pages={823-829},
abstract={Neural networks based on memristive devices1--3 have the ability to improve throughput and energy efficiency for machine learning4,5 and artificial intelligence6, especially in edge applications7--21. Because training a neural network model from scratch is costly in terms of hardware resources, time and energy, it is impractical to do it individually on billions of memristive neural networks distributed at the edge. A practical approach would be to download the synaptic weights obtained from the cloud training and program them directly into memristors for the commercialization of edge applications. Some post-tuning in memristor conductance could be done afterwards or during applications to adapt to specific situations. Therefore, in neural network applications, memristors require high-precision programmability to guarantee uniform and accurate performance across a large number of memristive networks22--28. This requires many distinguishable conductance levels on each memristive device, not only laboratory-made devices but also devices fabricated in factories. Analog memristors with many conductance states also benefit other applications, such as neural network training, scientific computing and even `mortal computing'25,29,30. Here we report 2,048 conductance levels achieved with memristors in fully integrated chips with 256{\thinspace}{\texttimes}{\thinspace}256 memristor arrays monolithically integrated on complementary metal--oxide--semiconductor (CMOS) circuits in a commercial foundry. We have identified the underlying physics that previously limited the number of conductance levels that could be achieved in memristors and developed electrical operation protocols to avoid such limitations. These results provide insights into the fundamental understanding of the microscopic picture of memristive switching as well as approaches to enable high-precision memristors for various applications.},
issn={1476-4687},
doi={10.1038/s41586-023-05759-5},
url={https://doi.org/10.1038/s41586-023-05759-5}
}

```